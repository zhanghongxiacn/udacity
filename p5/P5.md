
## P5: 从安然公司邮件中发现欺诈证据

### 向我们总结此项目的目标以及机器学习对于实现此目标有何帮助。作为答案的部分，提供一些数据集背景信息以及这些信息如何用于回答项目问题。你在获得数据时它们是否包含任何异常值，你是如何进行处理的？【相关标准项：“数据探索”，“异常值调查”】

此项目的目标是根据公开的安然财务和电子邮件数据集，找出有欺诈嫌疑的安然员工。
机器学习对于实现此目标有很大帮助，因为机器学习处理数据集的速度比人快，且他们可以找出人很难发现的趋势。下面给出一些安然财经邮件数据集背景信息。


#### 员工

数据集中一共有146位安然员工，其中18位有欺诈嫌疑。

#### 特征

14个财经特征，值都是美元
- salary
- deferral_payments
- total_payments
- loan_advances
- bonus
- restricted_stock_deferred
- deferred_income
- total_stock_value
- expenses
- exercised_stock_options
- other
- long_term_incentive
- restricted_stock
- director_fees

6个邮件特征，除‘email_address’值为文本字符串外，其他值为邮件数量
- to_messages
- email_address
- from_poi_to_this_person
- from_messages
- from_this_person_to_poi
- shared_receipt_with_poi

1个其他特征，值为布尔型，标识员工是否有欺诈嫌疑
- poi


#### 缺失值

21个特征中"poi"特征除外，其他20个特征都有缺失值（以"NaN"表示）。

缺失值估算为0

#### 异常值

- 通过画散点图识别出了一个特大异常值"Total"，参考enron61702insiderpay.pdf文档，该条记录由电子表格生成的汇总，不是一个人，因此在数据中我剔除了"Total"这行。
- 通过脚本识别出除poi之外所有其他特征取值都为'NaN'的记录'LOCKHART EUGENE E'，该条记录没有有用数据，故删除。

#### 综述

通过对该数据的分析，该数据集比较小，且嫌疑员工占比很小，也就是数据集很不平衡（imbalance）。这提供了我如下机器学习策略：

- 这个数据集很不平衡（imbalance）, 也就说明accuracy并不是很好的评估指标，所以选择precision和recall更好一些。
- 在交叉验证的时候，因为数据的不平衡性，因此选用Stratified Shuffle Split的方式将数据分为验证集和测试集。
- 数据样本比较少，算法评估使用的是test.py中的precision和recall，因此手动对算法参数调整。

#### 你最终在你的 POI 标识符中使用了什么特征，你使用了什么筛选过程来挑选它们？你是否需要进行任何缩放？为什么？作为任务的一部分，你应该尝试设计自己的特征，而非使用数据集中现成的——解释你尝试创建的特征及其基本原理。（你不一定要在最后的分析中使用它，而只设计并测试它）。在你的特征选择步骤，如果你使用了算法（如决策树），请也给出所使用特征的特征重要性；如果你使用了自动特征选择函数（如 SelectBest），请报告特征得分及你所选的参数值的原因。【相关标准项：“创建新特征”、“适当缩放特征”、“智能选择功能”】

#### 创建新特征

像数据集增加了两个特征，特征总数为23：

- fraction_from_poi
- fraction_to_poi

fraction_from_poi为'from_this_person_to_poi'与发邮件总数from_messages的比，fraction_to_poi为'from_poi_to_this_person' 与接收邮件总数to_messages的比。通过可视化，这两个新增特征好像能告诉我们一些信息，就是fraction_to_poi为0.2以下的基本都不是嫌疑员工。在SelectKBest得到的特征得分中，fraction_to_poi得分排第三，fraction_from_poi得分比较低比较靠后，而且在我尝试的朴素贝叶斯和决策树算法中，fraction_to_poi都作为最终选择的特征，而fraction_from_poi却没有被选为特征。

#### 特征缩放

因为我使用的是朴素贝叶斯以及决策树算法，都不涉及到距离，故不需要进行特征缩放。

#### 选择过程

我使用单变量特征选择过程，选择k-best 。通过SelectKBest得到所有特征的得分，手动调整'K'的取值，使得test.py执行结果中'f1'得分最高。

- 在朴树贝叶斯算法中，特征得分从高到低，依次添加特征，使得'f1'得分增加的特征保留直到得分最高，最终选择了9个得分比较高的特征。

- 在决策树算法中，使用决策树中的9个特征，通过列出决策树特征重要性，依次删除特征得分低且决策树特征重要性不高的特征使得'f1'得分最高。最终选择了3个重要特征。

#### 特征选择

我使用了下面3个特征：

第一个括号里的值为决策树重要性，第二个为SelectKBest得分
- 特征1: bonus (0.41936022) (30.652282305660439)
- 特征2: shared_receipt_with_poi (0.20454916) (10.669737359602689)
- 特征3: fraction_to_poi (0.37609062) (13.791413236761116)



### 你最终使用了什么算法？你还尝试了其他什么算法？不同算法之间的模型性能有何差异？【相关标准项：“选择算法”】

我尝试了两种算法:

- 朴素贝叶斯
- 决策树

朴素贝叶斯算法，我使用了9个特征，得到的最好结果：

Accuracy: 0.85840	Precision: 0.46349	Recall: 0.39350	F1: 0.42564	F2: 0.40575

决策树算法，我使用了3个特征，得到的最好结果：

Accuracy: 0.83036	Precision: 0.54007	Recall: 0.45150	F1: 0.49183	F2: 0.46681

从结果来看，决策树算法更好，最终选择了决策树算法。


### 调整算法的参数是什么意思，如果你不这样做会发生什么？你是如何调整特定算法的参数的？（一些算法没有需要调整的参数 – 如果你选择的算法是这种情况，指明并简要解释对于你最终未选择的模型或需要参数调整的不同模型，例如决策树分类器，你会怎么做）。【相关标准项：“调整算法”】

调整算法的参数是说使用一些方法调整算法参数使得算法性能最优。调整的方法有手动和自动，算法性能可以有很多种评估度量，比如精确率和召回率。不调整参数的话，算法性能也许会受影响，数据也不会被学习的很好，而且也不能够很好的预测新数据。

在这个项目中，我使用的是手动调整算法参数的方式。

手动调整如下参数：

- criterion=['gini', 'entropy']
- min_samples_split=[2,3,4,5,6]



|                                      | Accuracy  | Precision  | Recall |  F1    |   F2     |
| :---                                 |  :---:    |   :---:    |  :---: | :---:  | :---:    |
| random_state=100，criterion='gini'    | 0.81109   | 0.47794   |0.42250 |0.44851  |0.43253  |
| random_state=100，criterion='entropy' | 0.81091   | 0.47399   |0.36450 |0.41210  |0.38216  |
|random_state=100，min_samples_split=3  | 0.81827   | 0.50031   |0.41000 |0.45067  |0.42536  |
|random_state=100，min_samples_split=4  | 0.82509   | 0.52351   |0.42300 |0.46792  |0.43989  |
|random_state=100，min_samples_split=5  | 0.83182   | 0.54491   |0.45500 |0.49591  |0.47053  |
|random_state=100，min_samples_split=6  | 0.82336   | 0.51747   |0.42200 |0.46489  |0.43817  |


	
从上述表格中不难发现有些参数不使用默认参数，算法性能更好，当min_samples_split=5时，算法性能最优。

### 什么是验证，未正确执行情况下的典型错误是什么？你是如何验证你的分析的？【相关标准项：“验证策略”】

在机器学习中，我们经常会将数据集分为训练集（training set）跟测试集（testing set）这两个子集，前者用以建立模型（model），后者则用来评估该模型对未知样本进行预测时的精确度，正规的说法是泛化能力（generalization ability）。我们需要在测试集上进行验证，来确定训练集是否“过拟合”。没有正确执行验证会出现的典型错误是“过拟合”。

在我的 poi_id.py 文件中，我的数据被分成训练集与测试集。测试集的大小为数据集的30%，剩下的70%为训练集。生成的训练集用来做特征选择。

 
算法性能评估我使用了tester.py中的StratifiedShuffleSplit交叉验证方式。之所以选择Stratified Shuffle Split方式划分数据集，是因为数据集比较小，而且数据中poi显著少于非poi，而StratifiedShuffleSplit方式可以充分利用数据集，生成多组训练集和测试集，同时还能保证poi与非poi在原数据中的比例，这对数据很不平衡尤为重要，因为如果不保持类之间的配比，对于类比很不平衡的数据来说很可能就出现训练集中没有poi类，这显然是无效的学习。


### 给出至少 2 个评估度量并说明每个的平均性能。解释对用简单的语言表明算法性能的度量的解读。【相关标准项：“评估度量的使用”】

两个重要的评估度量是精确率与回召率，还有一个者二者综合评估度量F1。我的决策树分类器平均精确率是0.54007，平均回召率是0.45150，平均F1得分是0.49183。这代表什么意思呢?

- 精确率是指分类器预测(POI vs. non-POI)正确的程度。也就是在预测出POI的安然员工中，预测正确的占多少,公式表达就是TP/(TP+FP)。
- 召回率是指在所有标识为POI的安然员工中，分类器正确识别出POI的数量占多少,公式表达就是TP/(TP+FN)。
- F1是精确率和召回率的调和平均,公式表达就是P*R/2(P+R)，其中P和R分别为 precision 和 recall。

精确率可以理解为宁可放过，也不可误杀。而召回率可以理解为宁可误杀也不可放过。

